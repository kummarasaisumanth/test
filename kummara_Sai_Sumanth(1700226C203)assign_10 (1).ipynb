{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kummara_Sai_Sumanth(1700226C203)assign-10.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AelBWB9VKQXd",
        "colab_type": "text"
      },
      "source": [
        "multiple regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFpFThlbkjRz",
        "colab_type": "code",
        "outputId": "d9d6f246-427b-49ab-d61c-7c4ad3dc0faf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        }
      },
      "source": [
        "#logistic regression\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "n_features = 4\n",
        "X = []\n",
        "for i in range(n_features):\n",
        "  X_i = scipy.stats.norm.rvs(0, 1, 100)\n",
        "  X.append(X_i)\n",
        "#print(X)\n",
        "a1 = (np.exp(1 + (0.5 * X[0]) + (0.4 * X[1]) + (0.3 * X[2]) + (0.5 * X[3]))/(1 + np.exp(1 + (0.5 * X[0]) + (0.4 * X[1]) + (0.3 * X[2]) + (0.5 * X[3]))))\n",
        "#print(a1)\n",
        "y1 = []\n",
        "for i in a1:\n",
        "  if (i>=0.5):\n",
        "    y1.append(1)\n",
        "  else:\n",
        "    y1.append(0)\n",
        "#print(y1)\n",
        "data_lr = {'X0': X[0],'X1':X[1],'X2':X[2],'X3':X[3],'Y': y1 }\n",
        "dflr = pd.DataFrame(data_lr)\n",
        "print(dflr.head())\n",
        "print(dflr.tail())\n",
        "print(dflr.info())\n",
        "print(dflr.describe())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         X0        X1        X2        X3  Y\n",
            "0  0.782801  0.689161  1.157223  1.206988  1\n",
            "1  0.254343 -0.742981  1.529536 -0.546128  1\n",
            "2 -1.415175 -0.295573 -0.009374 -0.611898  0\n",
            "3 -0.467611 -2.250639  0.773821 -0.542211  0\n",
            "4 -0.683823 -0.756647  0.145695 -0.076641  1\n",
            "          X0        X1        X2        X3  Y\n",
            "95 -0.328263 -0.666448 -0.875486  0.395951  1\n",
            "96 -0.512497 -0.205066 -0.348911 -1.690471  0\n",
            "97 -1.120547  0.760680 -1.427585  1.086683  1\n",
            "98 -0.094963  1.027191  0.313411  0.825740  1\n",
            "99 -1.363683  0.943590  0.130307  0.041982  1\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   X0      100 non-null    float64\n",
            " 1   X1      100 non-null    float64\n",
            " 2   X2      100 non-null    float64\n",
            " 3   X3      100 non-null    float64\n",
            " 4   Y       100 non-null    int64  \n",
            "dtypes: float64(4), int64(1)\n",
            "memory usage: 4.0 KB\n",
            "None\n",
            "               X0          X1          X2          X3           Y\n",
            "count  100.000000  100.000000  100.000000  100.000000  100.000000\n",
            "mean    -0.019390    0.060793    0.137145   -0.005170    0.870000\n",
            "std      0.869577    1.052365    0.985412    1.030509    0.337998\n",
            "min     -1.981592   -2.250639   -2.320957   -2.864379    0.000000\n",
            "25%     -0.667507   -0.723467   -0.575677   -0.772443    1.000000\n",
            "50%      0.060937    0.071655    0.155652    0.009213    1.000000\n",
            "75%      0.625340    0.861380    0.781615    0.737740    1.000000\n",
            "max      1.833633    2.445516    2.396602    2.893819    1.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-Rv6X7ul5QT",
        "colab_type": "code",
        "outputId": "1144c5b3-05da-4651-a030-1ca5178aa942",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        }
      },
      "source": [
        "# multiple regression\n",
        "import numpy as np\n",
        "import scipy\n",
        "import random\n",
        "from scipy.stats import norm\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "random.seed(1)\n",
        "mx1 = scipy.stats.norm.rvs(0, 1, 100)\n",
        "mx2 = scipy.stats.norm.rvs(0.5, 1.05,100)\n",
        "mx3 = scipy.stats.norm.rvs(0.2, 0.95,100)\n",
        "eps = scipy.stats.norm.rvs(0,0.25,100)\n",
        "y = eps + 0.5*mx1 + 0.8*mx2 + 0.4*mx3\n",
        "data_gen = {'mx1':mx1,'mx2':mx2,'mx3':mx3,'y':y}\n",
        "dtf = pd.DataFrame(data_gen)\n",
        "print(dtf.head())\n",
        "print(dtf.tail())\n",
        "print(dtf.info())\n",
        "print(dtf.describe())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        mx1       mx2       mx3         y\n",
            "0  1.057713  0.810284 -0.984857  1.001701\n",
            "1 -1.721496 -1.551742 -0.255153 -2.070894\n",
            "2 -0.080501  0.347325 -0.606797  0.553491\n",
            "3 -0.285551  1.789591  0.741920  1.432707\n",
            "4  0.360972  1.539771 -0.582724  1.345284\n",
            "         mx1       mx2       mx3         y\n",
            "95 -1.591072  1.374754  0.425587  0.113081\n",
            "96 -1.102961 -0.416650  1.177826 -0.514597\n",
            "97  0.853734  0.846185  0.038451  1.386007\n",
            "98 -1.702101  0.445329 -0.761162 -0.974224\n",
            "99  1.586373  2.093506  0.411639  2.043246\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 4 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   mx1     100 non-null    float64\n",
            " 1   mx2     100 non-null    float64\n",
            " 2   mx3     100 non-null    float64\n",
            " 3   y       100 non-null    float64\n",
            "dtypes: float64(4)\n",
            "memory usage: 3.2 KB\n",
            "None\n",
            "              mx1         mx2         mx3           y\n",
            "count  100.000000  100.000000  100.000000  100.000000\n",
            "mean     0.033092    0.430617    0.391387    0.493305\n",
            "std      0.942206    1.118606    0.965932    1.152514\n",
            "min     -2.079781   -2.188973   -2.262697   -2.070894\n",
            "25%     -0.534894   -0.271496   -0.309092   -0.321841\n",
            "50%      0.028229    0.419599    0.454591    0.627341\n",
            "75%      0.769107    1.211014    1.046400    1.395840\n",
            "max      2.553609    2.688703    2.793995    3.383631\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bF8huKL7KO7Z",
        "colab_type": "text"
      },
      "source": [
        "k-means clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZrojFVZA1pr",
        "colab_type": "code",
        "outputId": "72c347ed-c128-46d4-e404-59454ccf4dc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 828
        }
      },
      "source": [
        "#k-means\n",
        "import matplotlib.pyplot as plt\n",
        "X_a= -2 * np.random.rand(100,2)\n",
        "X_b = 1 + 2 * np.random.rand(50,2)\n",
        "X_a[50:100, :] = X_b\n",
        "plt.scatter(X_a[ : , 0], X_a[ :, 1], s = 50)\n",
        "plt.show()\n",
        "kmeansdatagen = {'X0': X_a[:,0],'X1':X_a[:,1]}\n",
        "dfkmeans = pd.DataFrame(kmeansdatagen)\n",
        "print(dfkmeans.head())\n",
        "print(dfkmeans.tail())\n",
        "print(dfkmeans.info())\n",
        "print(dfkmeans.describe())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "         X0        X1\n",
            "0 -0.502746 -0.528372\n",
            "1 -0.921589 -0.268774\n",
            "2 -0.373599 -1.021586\n",
            "3 -0.113456 -0.024942\n",
            "4 -0.815413 -0.998299\n",
            "          X0        X1\n",
            "95  1.275650  1.154022\n",
            "96  1.368534  1.321313\n",
            "97  2.047909  2.948418\n",
            "98  1.408102  2.990087\n",
            "99  2.488614  1.781273\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   X0      100 non-null    float64\n",
            " 1   X1      100 non-null    float64\n",
            "dtypes: float64(2)\n",
            "memory usage: 1.7 KB\n",
            "None\n",
            "               X0          X1\n",
            "count  100.000000  100.000000\n",
            "mean     0.617333    0.502476\n",
            "std      1.539709    1.602142\n",
            "min     -1.995814   -1.965581\n",
            "25%     -0.670708   -0.985792\n",
            "50%      0.518707    0.495535\n",
            "75%      2.149252    1.985295\n",
            "max      2.921602    2.990087\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7HMw5Tkrq8G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-u07unkrr5K",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMMRFmF1KNjR",
        "colab_type": "text"
      },
      "source": [
        "linear regression using gradient desent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HI1sEIaCEUDq",
        "colab_type": "code",
        "outputId": "275dc1f4-f166-44ec-edf7-acd330b59d72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import pandas as pd\n",
        "X = dtf.iloc[:,0].values\n",
        "#print(X)\n",
        "y = dtf.iloc[:,3].values\n",
        "lx1 = 0\n",
        "lx0 = 0\n",
        "y1 = 0.001\n",
        "epo = 100\n",
        " \n",
        "n = float(len(X))\n",
        "for i in range(epo):\n",
        "  ypredic = lx1*X + lx0\n",
        "  lossfunc = np.sum(ypredic - y1)**2\n",
        "  d1 = (-2/n) * sum(X * (y - ypredic))\n",
        "  d0 = (-2/n) * sum(y - ypredic)\n",
        "  lx1 = lx1 - (l*d1)\n",
        "  lx0 = lx0 - (l*d0)\n",
        "\n",
        "print(lx1,lx0)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.10054371326147431 0.08918381768924051\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQ1WWKBQKLwC",
        "colab_type": "text"
      },
      "source": [
        "logistic regression using gredient desent "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wM70W2tUG4uy",
        "colab_type": "code",
        "outputId": "619864a6-8475-41a7-b2ce-a1b51ee1913b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "X1 = dflr.iloc[:,0:4].values\n",
        "y1 = dflr.iloc[:,4].values\n",
        "\n",
        "def sigmoid(Z):\n",
        "  return 1 /(1+np.exp(-Z))\n",
        "\n",
        "def loss(y1,y_hat):\n",
        "  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat)))\n",
        "\n",
        "W = np.zeros((4,1))\n",
        "b = np.zeros((1,1))\n",
        "\n",
        "m = len(y1)\n",
        "lr = 0.001\n",
        "for epoch in range(1000):\n",
        "  Z = np.matmul(X1,W)+b\n",
        "  A = sigmoid(Z)\n",
        "  logistic_loss = loss(y1,A)\n",
        "  dz = A - y1\n",
        "  dw = 1/m * np.matmul(X1.T,dz)\n",
        "  db = np.sum(dz)\n",
        "\n",
        "  W = W - lr*dw\n",
        "  b = b - lr*db\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(logistic_loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6931471805599453\n",
            "0.3861303837140154\n",
            "0.3858795398037182\n",
            "0.38563407070431577\n",
            "0.3853938747128461\n",
            "0.3851588517489842\n",
            "0.3849289033163202\n",
            "0.3847039324659183\n",
            "0.38448384376208694\n",
            "0.38426854325028\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1BQLYbUJ_3M",
        "colab_type": "text"
      },
      "source": [
        "Linear regression using L1 regularisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXe5oR2hH4K8",
        "colab_type": "code",
        "outputId": "3d6dd06b-4c4a-4131-dc1d-7e803f3d271c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X = dtf.iloc[:,0].values\n",
        "#print(X)\n",
        "y = dtf.iloc[:,3].values\n",
        "b1 = 0\n",
        "b0 = 0\n",
        "l = 0.001\n",
        "epochs = 100\n",
        "lam = 0.1\n",
        " \n",
        "n = float(len(X))\n",
        "for i in range(epochs):\n",
        "  y_p = b1*X + b0\n",
        "  loss = np.sum(y_p - y1)**2 + (lam * b1)\n",
        "  d1 = (-2/n) * sum(X * (y - y_p)) + lam\n",
        "  d0 = (-2/n) * sum(y - y_p)\n",
        "  b1 = b1 - (l*d1)\n",
        "  b0 = b0 - (l*d0)\n",
        "\n",
        "print(b1,b0)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.09136680315141227 0.08921281733784203\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EPe5YpiJ5Or",
        "colab_type": "text"
      },
      "source": [
        "Linear regression using L2 regularistion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8y1jt46II4W",
        "colab_type": "code",
        "outputId": "5bbe4a50-0051-496f-e4fa-20dcd2f826e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X = dtf.iloc[:,0].values\n",
        "#print(X)\n",
        "y = dtf.iloc[:,3].values\n",
        "b1 = 0\n",
        "b0 = 0\n",
        "l = 0.001\n",
        "epochs = 100\n",
        "lam = 0.1\n",
        " \n",
        "n = float(len(X))\n",
        "for i in range(epochs):\n",
        "  y_p = b1*X + b0\n",
        "  loss = np.sum(y_p - y1)**2 + ((lam/2) * b1)\n",
        "  d1 = (-2/n) * sum(X * (y - y_p)) + (lam *b1)\n",
        "  d0 = (-2/n) * sum(y - y_p)\n",
        "  b1 = b1 - (l*d1)\n",
        "  b0 = b0 - (l*d0)\n",
        "\n",
        "print(b1,b0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.10006102543673086 0.08918484281179606\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4kMLNwmJvPI",
        "colab_type": "text"
      },
      "source": [
        "logistic regression using L1 regularisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KabBvwAIOic",
        "colab_type": "code",
        "outputId": "6997eee5-756a-46e6-f7af-562a4bf313bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        }
      },
      "source": [
        "X1 = dflr.iloc[:,0:4].values\n",
        "y1 = dflr.iloc[:,4].values\n",
        "lam = 0.1\n",
        "def sigmoid(Z):\n",
        "  return 1 /(1+np.exp(-Z))\n",
        "\n",
        "def loss(y1,y_hat):\n",
        "  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat))) + (lam * (np.sum(W)))\n",
        "\n",
        "W = np.zeros((4,1))\n",
        "b = np.zeros((1,1))\n",
        "\n",
        "m = len(y1)\n",
        "lr = 0.001\n",
        "for epoch in range(1000):\n",
        "  Z = np.matmul(X1,W)+b\n",
        "  A = sigmoid(Z)\n",
        "  logistic_loss = loss(y1,A)\n",
        "  dz = A - y1\n",
        "  dw = 1/m * np.matmul(X1.T,dz) + lam\n",
        "  db = np.sum(dz)\n",
        "\n",
        "  W = W - lr*dw\n",
        "  b = b - lr*dw\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(logistic_loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6931471805599453\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-9c948626532f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m   \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mlogistic_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (100,100) (4,100) "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VCvK3d8Jhyk",
        "colab_type": "text"
      },
      "source": [
        "logistic regression using L2 regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S631oHm3ITGW",
        "colab_type": "code",
        "outputId": "8b953447-a25f-4fb2-e0fc-13a15d1730ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "X1 = dflr.iloc[:,0:4].values\n",
        "y1 = dflr.iloc[:,4].values\n",
        "lam = 0.1\n",
        "def sigmoid(Z):\n",
        "  return 1 /(1+np.exp(-Z))\n",
        "\n",
        "def loss(y1,y_hat):\n",
        "  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat))) + (lam * (np.sum(np.square(W))))\n",
        "\n",
        "W = np.zeros((4,1))\n",
        "b = np.zeros((1,1))\n",
        "\n",
        "m = len(y1)\n",
        "lr = 0.001\n",
        "for epoch in range(1000):\n",
        "  Z = np.matmul(X1,W)+b\n",
        "  A = sigmoid(Z)\n",
        "  logistic_loss = loss(y1,A)\n",
        "  dz = A - y1\n",
        "  dw = 1/m * np.matmul(X1.T,dz) + lam * W\n",
        "  db = np.sum(dz)\n",
        "\n",
        "  W = W - lr*dw\n",
        "  b = b - lr*db\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(logistic_loss)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6931471805599453\n",
            "0.38638546286619085\n",
            "0.3868789532324436\n",
            "0.38783680847387925\n",
            "0.3892300483392461\n",
            "0.3910310329055937\n",
            "0.3932134085994512\n",
            "0.3957520562720446\n",
            "0.39862304123956843\n",
            "0.4018035652048819\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9xFnstGJbUQ",
        "colab_type": "text"
      },
      "source": [
        "k means clustring algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBjxvTbhIaO6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class K_Means:\n",
        "    def __init__(self, k=2, tol=0.001, max_iter=300):\n",
        "        self.k = k\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "    def fit(self,data):\n",
        "\n",
        "        self.centroids = {}\n",
        "\n",
        "        for i in range(self.k):\n",
        "            self.centroids[i] = data[i]\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            self.classifications = {}\n",
        "\n",
        "            for i in range(self.k):\n",
        "                self.classifications[i] = []\n",
        "\n",
        "            for featureset in X:\n",
        "                distances = [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids]\n",
        "                classification = distances.index(min(distances))\n",
        "                self.classifications[classification].append(featureset)\n",
        "\n",
        "            prev_centroids = dict(self.centroids)\n",
        "\n",
        "            for classification in self.classifications:\n",
        "                self.centroids[classification] = np.average(self.classifications[classification],axis=0)\n",
        "\n",
        "            optimized = True\n",
        "\n",
        "            for c in self.centroids:\n",
        "                original_centroid = prev_centroids[c]\n",
        "                current_centroid = self.centroids[c]\n",
        "                if np.sum((current_centroid-original_centroid)/original_centroid*100.0) > self.tol:\n",
        "                    print(np.sum((current_centroid-original_centroid)/original_centroid*100.0))\n",
        "                    optimized = False\n",
        "\n",
        "            if optimized:\n",
        "                break\n",
        "\n",
        "    def predict(self,data):\n",
        "        distances = [np.linalg.norm(data-self.centroids[centroid]) for centroid in self.centroids]\n",
        "        classification = distances.index(min(distances))\n",
        "        return classification\n",
        "        \n",
        "colors = 10*[\"g\",\"r\",\"c\",\"b\",\"k\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szSQaH4VIeY3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df3.iloc[:,0:2].values\n",
        "clf = K_Means()\n",
        "clf.fit(X)\n",
        "\n",
        "for centroid in clf.centroids:\n",
        "    plt.scatter(clf.centroids[centroid][0], clf.centroids[centroid][1],\n",
        "                marker=\"o\", color=\"k\", s=150, linewidths=5)\n",
        "\n",
        "for classification in clf.classifications:\n",
        "    color = colors[classification]\n",
        "    for featureset in clf.classifications[classification]:\n",
        "        plt.scatter(featureset[0], featureset[1], marker=\"x\", color=color, s=150, linewidths=5)\n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ne6wwneaJMzJ",
        "colab_type": "text"
      },
      "source": [
        "linear regession with oops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pYYkFS0ImvL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class LinearRegressionModel():\n",
        "\n",
        "    def __init__(self, dataset, learning_rate, num_iterations):\n",
        "        self.dataset = np.array(dataset)\n",
        "        self.b = 0  \n",
        "        self.m = 0  \n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.M = len(self.dataset)\n",
        "        self.total_error = 0\n",
        "\n",
        "    def apply_gradient_descent(self):\n",
        "        for i in range(self.num_iterations):\n",
        "            self.do_gradient_step()\n",
        "\n",
        "    def do_gradient_step(self):\n",
        "        b_summation = 0\n",
        "        m_summation = 0\n",
        "        for i in range(self.M):\n",
        "            x_value = self.dataset[i, 0]\n",
        "            y_value = self.dataset[i, 1]\n",
        "            b_summation += (((self.m * x_value) + self.b) - y_value) \n",
        "            m_summation += (((self.m * x_value) + self.b) - y_value) * x_value\n",
        "        self.b = self.b - (self.learning_rate * (1/self.M) * b_summation)\n",
        "        self.m = self.m - (self.learning_rate * (1/self.M) * m_summation)\n",
        "      \n",
        "    def compute_error(self):\n",
        "        for i in range(self.M):\n",
        "            x_value = self.dataset[i, 0]\n",
        "            y_value = self.dataset[i, 1]\n",
        "            self.total_error += ((self.m * x_value) + self.b) - y_value\n",
        "        return self.total_error\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Results: b: {}, m: {}, Final Total error: {}\".format(round(self.b, 2), round(self.m, 2), round(self.compute_error(), 2))\n",
        "\n",
        "    def get_prediction_based_on(self, x):\n",
        "        return round(float((self.m * x) + self.b), 2) # Type: Numpy float.\n",
        "\n",
        "def main():\n",
        "    school_dataset = np.genfromtxt(DATASET_PATH, delimiter=\",\")\n",
        "    lr = LinearRegressionModel(school_dataset, 0.0001, 1000)\n",
        "    lr.apply_gradient_descent()\n",
        "    hours = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "    for hour in hours:\n",
        "        print(\"Studied {} hours and got {} points.\".format(hour, lr.get_prediction_based_on(hour)))\n",
        "    print(lr)\n",
        "\n",
        "if __name__ == \"__main__\": main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQcqYqRFJHcy",
        "colab_type": "text"
      },
      "source": [
        "logistic regression with oops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq_WNekGIx0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LogisticRegression:\n",
        "  def __init__(self, learning_rate, num_iters, fit_intercept = True, verbose = False):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.num_iters = num_iters\n",
        "    self.fit_intercept = fit_intercept\n",
        "    self.verbose = verbose\n",
        "  def __add_intercept(self, X):\n",
        "    intercept = np.ones((X.shape[0],1))\n",
        "    return np.concatenate((intercept,X),axis=1)\n",
        "  def __sigmoid(self,z):\n",
        "    return 1/(1+np.exp(-z))\n",
        "  def __loss(self, h, y):\n",
        "    return (-y * np.log(h) - (1-y) * np.log(1-h)).mean()\n",
        "  \n",
        "  def fit(self,X,y):\n",
        "    if self.fit_intercept:\n",
        "      X = self.__add_intercept(X)\n",
        "    self.theta = np.zeros(X.shape[1])\n",
        "    \n",
        "    for i in range(self.num_iters):\n",
        "      z = np.dot(X,self.theta)\n",
        "      h = self.__sigmoid(z)\n",
        "      gradient = np.dot(X.T,(h-y))/y.size\n",
        "      \n",
        "      self.theta -= self.learning_rate * gradient\n",
        "      \n",
        "      z = np.dot(X,self.theta)\n",
        "      h = self.__sigmoid(z)\n",
        "      loss = self.__loss(h,y)\n",
        "      \n",
        "      if self.verbose == True and i % 1000 == 0:\n",
        "        print(f'Loss: {loss}\\t')\n",
        "  def predict_probability(self,X):\n",
        "    if self.fit_intercept:\n",
        "      X = self.__add_intercept(X)\n",
        "    return self.__sigmoid(np.dot(X,self.theta))\n",
        "  def predict(self,X):\n",
        "    return (self.predict_probability(X).round())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_fFhy5eJD5c",
        "colab_type": "text"
      },
      "source": [
        "k means with oops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVqFbGw0I2wP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class K_Means:\n",
        "    def __init__(self, k=2, tol=0.001, max_iter=300):\n",
        "        self.k = k\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "    def fit(self,data):\n",
        "\n",
        "        self.centroids = {}\n",
        "\n",
        "        for i in range(self.k):\n",
        "            self.centroids[i] = data[i]\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            self.classifications = {}\n",
        "\n",
        "            for i in range(self.k):\n",
        "                self.classifications[i] = []\n",
        "\n",
        "            for featureset in X:\n",
        "                distances = [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids]\n",
        "                classification = distances.index(min(distances))\n",
        "                self.classifications[classification].append(featureset)\n",
        "\n",
        "            prev_centroids = dict(self.centroids)\n",
        "\n",
        "            for classification in self.classifications:\n",
        "                self.centroids[classification] = np.average(self.classifications[classification],axis=0)\n",
        "\n",
        "            optimized = True\n",
        "\n",
        "            for c in self.centroids:\n",
        "                original_centroid = prev_centroids[c]\n",
        "                current_centroid = self.centroids[c]\n",
        "                if np.sum((current_centroid-original_centroid)/original_centroid*100.0) > self.tol:\n",
        "                    print(np.sum((current_centroid-original_centroid)/original_centroid*100.0))\n",
        "                    optimized = False\n",
        "\n",
        "            if optimized:\n",
        "                break\n",
        "\n",
        "    def predict(self,data):\n",
        "        distances = [np.linalg.norm(data-self.centroids[centroid]) for centroid in self.centroids]\n",
        "        classification = distances.index(min(distances))\n",
        "        return classification\n",
        "        \n",
        "colors = 10*[\"g\",\"r\",\"c\",\"b\",\"k\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPy2RRYII-EI",
        "colab_type": "text"
      },
      "source": [
        "question 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmQdE9bjzXBO",
        "colab_type": "code",
        "outputId": "72eefa0c-f92b-49b6-f9b8-10b775963125",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import numpy as np\n",
        "#from sklearn import preprocessing and cross_validation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(123)\n",
        "\n",
        "allwalks = []\n",
        "\n",
        "for p in range(250):\n",
        "    randwalk = [0]\n",
        "    for o in range(100):\n",
        "        steps = randwalk[-1]\n",
        "        dice = np.random.randint(1,7)\n",
        "        if dice <= 2 :\n",
        "            steps= max(0, steps - 1)\n",
        "\n",
        "        elif dice<=5:\n",
        "            steps += 1\n",
        "\n",
        "        else:\n",
        "            steps = steps + np.random.randint(1,7)\n",
        "        \n",
        "    print(steps)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "5\n",
            "1\n",
            "0\n",
            "2\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "4\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "4\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "2\n",
            "1\n",
            "0\n",
            "6\n",
            "0\n",
            "0\n",
            "1\n",
            "6\n",
            "0\n",
            "3\n",
            "4\n",
            "1\n",
            "1\n",
            "6\n",
            "1\n",
            "0\n",
            "5\n",
            "1\n",
            "0\n",
            "1\n",
            "5\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "5\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "5\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "4\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "3\n",
            "3\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "6\n",
            "1\n",
            "2\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "3\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "4\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "4\n",
            "1\n",
            "1\n",
            "1\n",
            "4\n",
            "1\n",
            "2\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}